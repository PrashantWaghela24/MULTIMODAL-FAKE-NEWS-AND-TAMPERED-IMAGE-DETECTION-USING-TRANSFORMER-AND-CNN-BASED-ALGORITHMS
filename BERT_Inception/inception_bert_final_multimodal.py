# -*- coding: utf-8 -*-
"""Inception_BERT_Final_Multimodal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rgD3HyQKPUxddu7lLtPc4tfsQ2EOEupj

Mounting Google Drive to access dataset and save created model
"""

from google.colab import drive
drive.mount('/content/drive')

"""Library Installation"""

!pip install tensorflow-text

"""Importing various necessary libraries"""

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
import pandas as pd
import numpy as np
from sklearn import preprocessing
import keras

"""Read the csv data file"""

df = pd.read_csv('/content/drive/MyDrive/FinalCleanData.csv')
df=pd.DataFrame(df)

"""Custom for loop to extract image path from Google Drive location"""

old_filelength=38520
train_split_val = 29699    #This value is 80% of entire dataset length 
FilePath_arr=[]
img_number_arr=[]
for i in range(0,len(df)):
    if((i <= train_split_val) & (df.iat[i,5] == 0)):
        FilePath_arr.append('/content/drive/MyDrive/images_resized/train_resized/fake_resized/image-' + str(df.iat[i,1]) + '.jpg')
        img_number_arr.append(df.iat[i,1])
    elif((i <= train_split_val) & (df.iat[i,5] == 1)):
        FilePath_arr.append('/content/drive/MyDrive/images_resized/train_resized/true_resized/image-' + str(df.iat[i,1]) + '.jpg')
        img_number_arr.append(df.iat[i,1])
    elif((i > train_split_val) & (df.iat[i,5] == 0)):
        FilePath_arr.append('/content/drive/MyDrive/images_resized/test_resized/fake_resized/image-' + str(df.iat[i,1]) + '.jpg')
        img_number_arr.append(df.iat[i,1])
    elif((i > train_split_val) & (df.iat[i,5] == 1)):
        FilePath_arr.append('/content/drive/MyDrive/images_resized/test_resized/true_resized/image-' + str(df.iat[i,1]) + '.jpg')
        img_number_arr.append(df.iat[i,1])

"""Creating new dataframe which includes the image path on google drive. This is created using the array generated in the above code."""

image_number = np.array(img_number_arr)
Path = np.array(FilePath_arr)
new_df = pd.DataFrame({'image_number': image_number, 'Path': Path}, columns=['image_number', 'Path'])
print(new_df)

"""Inner joining the new_df with merged_df using the key 'image_number' to obtain a final dataframe."""

merged_df=pd.merge(left=new_df, right=df, on='image_number', how='inner')
print(merged_df)
#merged_df.to_csv('/content/drive/MyDrive/MergedData.csv')

"""Porter Stemming implementation to obtain clean pre-processed news text."""

#Importing necessary libraries

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras import layers

voc_size=10000

import nltk
import re
from nltk.corpus import stopwords
nltk.download('stopwords')

#News text is cleaned using regular expression which removes special characters from the text.
#Porter Stemming is used to remove stop words from the text to simplify it for the model.

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
corpus = []
for i in range(0, len(merged_df)):
    print(i)
    news = re.sub('[^a-zA-Z0-9]', ' ', merged_df.iat[i,5])
    news = news.lower()
    news = news.split()
    
    news = [ps.stem(word) for word in news if not word in stopwords.words('english')]
    news = ' '.join(news)
    corpus.append(news)

"""Adding the preprocessed text column in the main dataframe"""

merged_df['preprocessed_newsText']=pd.Series(corpus)
merged_df.head()
#merged_df.to_csv('/content/drive/MyDrive/FinalProcessed_MergedData.csv')

"""The above created final dataframe is firstly saved in google drive and then read using pandas read_csv function."""

import pandas as pd
import numpy as np

df_final = pd.read_csv('/content/drive/MyDrive/FinalProcessed_MergedData.csv')
print(df_final.head())

"""Removal of unnecessary columns from the final dataframe."""

df_final.drop(['Unnamed: 0','index','domain','img_url','news_title'], inplace=True, axis=1)

df_final.head()

"""Importing BERT and text processing related necessary libraries"""

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot

"""Downloading the pretrained BERT preprocessor and encoder."""

bert_preprocess = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
bert_encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4")


#XML_RoBERTa
#bert_preprocess = hub.KerasLayer("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1")
#bert_encoder = hub.KerasLayer("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1")

"""Importing a few more necessary libraries"""

import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
from sklearn import preprocessing
import keras
import numpy as np
from keras import layers

"""Importing model building libraries """

from keras import layers
from keras.models import Model

from keras.layers import Input,Lambda, Dense, Flatten

from keras.layers import Input,Lambda, Dense
from keras.models import Model
import keras.backend as K

"""Imporing image processing related libraries"""

from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
import matplotlib.pyplot as plt

"""Creating a custom for loop to convert data into npz array for image path, news text, and target column."""

import cv2

npz_paths = []

for i, row in df_final.iterrows():
  picture_path = row['Path']
  print(i)
  npz_path = picture_path.split('.')[0] + '.npz'
  npz_paths.append(npz_path)

  pic_bgr_arr = cv2.imread(picture_path)
  pic_rgb_arr = cv2.cvtColor(pic_bgr_arr, cv2.COLOR_BGR2RGB)
  
  preprocessed_news = row['preprocessed_newsText']
  news = np.array(preprocessed_news)

  fake = row['is_fake']
  np.savez_compressed(npz_path, pic=pic_rgb_arr, news=news, fake=fake)

"""Adding 'npz_paths' array to the final dataframe and saving it in Google Drive"""

df_final['NPZ_Path'] = pd.Series(npz_paths)

df_final
#df_final.to_csv('/content/drive/MyDrive/df_final_NPZ.csv')

"""Read final dataframe for model implementation"""

df_final = pd.read_csv('/content/drive/MyDrive/df_FinalClean_NPZData.csv')

"""Custom function to split train, validation, and test data and divide them into news, image, and target numpy arrays separately."""

#The function fetches image path, news text, and target column from the created npz path. 

def get_X_y(df):

  X_pic, X_news= [], []
  y=[]

  for name in df['NPZ_Path_new']:
    loaded_npz = np.load(name)
    print(name)
    pic = loaded_npz['pic']
    X_pic.append(pic)

    news = loaded_npz['news']
    X_news.append(news)
    
    y.append(loaded_npz['fake'])

  X_pic, X_news = np.array(X_pic), np.array(X_news)
  y = np.array(y)

  return (X_pic, X_news), y

"""Generating train data using the above created function 'get_X_y'"""

# Get the training data

(X_train_pic, X_train_news), y_train = get_X_y(df_final[0:28000])

# Check shape of all arrays

(X_train_pic.shape, X_train_news.shape), y_train.shape

"""Generating validation data using the above created function 'get_X_y'"""

# Get the validation data

(X_val_pic, X_val_news), y_val = get_X_y(df_final[28000:34000])

# Check shape of all arrays

(X_val_pic.shape, X_val_news.shape), y_val.shape

"""Generating test data using the above created function 'get_X_y'"""

# Get the test data

(X_test_pic, X_test_news), y_test = get_X_y(df_final[34000:37113])

# Check shape of all arrays

(X_test_pic.shape, X_test_news.shape), y_test.shape

"""Importing necessary Inception V3 related libraries"""

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
import matplotlib.pyplot as plt

"""Define Image Size and Inception Parameters"""

IMAGE_SIZE = [200, 200]
inception = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)

"""Making Pre Trained weights False to train it properly as per the unique labelled data"""

# don't train existing weights
for layer in inception.layers:
    layer.trainable = False

# useful for getting number of output classes
folders = glob('/content/drive/MyDrive/images_resized_new/train_resized/*')
print(folders)

"""Creating a combinatory BERT and CNN Multimodal algorithm for simultaneous text and image data classification."""

# Define the Inception Stream

x = tf.keras.layers.Flatten()(inception.output)
#x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dropout(0.1)(x)
x = tf.keras.layers.Dense(512, activation='relu')(x)
x = tf.keras.Model(inputs=inception.input, outputs=x)

# Define the BERT Model

text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)
y = tf.keras.layers.Flatten()(outputs['pooled_output'])
y = tf.keras.layers.Dropout(0.5, name="dropout2")(y)
y = tf.keras.layers.Dense(768, activation='relu')(y)
y = tf.keras.Model(inputs=[text_input], outputs = [y])


# Concatenate the two streams together

combined = tf.keras.layers.concatenate([x.output, y.output])
z = layers.Dense(64, activation="relu")(combined)
#z = layers.Dropout(0.2, name="dropout3")(z)
#z = layers.Dense(64, activation="relu")(z)


# Define output node of 1 categorical neuron (classification task)

z = tf.keras.layers.Dense(1, activation="sigmoid")(combined)


# Define the final model

model = Model(inputs=[x.input, y.input], outputs=z)

"""Checking Model Summary and various Parameters"""

model.summary()

"""Model Compilation"""

# Compile the model with Adam optimizer and defining accuracy, precision, and recall metrics

from keras.optimizers import Adam

METRICS = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall')
]

optimizer = Adam(learning_rate=0.001)

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=METRICS)

"""Early Stopping and ModelCheckpoint Implementation to save the best model"""

# Create a model saving callback and train for 10 epochs

from keras.callbacks import ModelCheckpoint, EarlyStopping
import os
import scipy
import pandas as pd
import numpy as np

es = EarlyStopping(patience=3)
cp = ModelCheckpoint('/content/drive/MyDrive/model_latest_BERT_Inception', save_best_only=True, save_weights_only=True)
cb=[es,cp]

"""Model Training Step"""

history = model.fit(x=[X_train_pic, X_train_news], y=y_train, batch_size =128, epochs=10, validation_data=([X_val_pic, X_val_news], y_val), verbose=1, callbacks=[cb])

"""Loss and Accuracy Plots using MatPlotLib to observe graphical behaviour of loss and accuracy metrics"""

import matplotlib.pyplot as plt

# plot the loss
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.title("Training vs Validation Loss Plot")
plt.show()
plt.savefig('LossVal_loss')

# plot the accuracy
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend()
plt.title("Training vs Validation Accuracy Plot")
plt.show()
plt.savefig('AccVal_acc')

"""Model Saving on Google Drive"""

# save the model as a h5 file
from tensorflow.keras.models import load_model

model.save('/content/drive/MyDrive/BERT_Inception_Multi_Modal_Final')

"""Loading the Saved Model"""

from keras.models import load_model

loaded_model = load_model('/content/drive/MyDrive/BERT_Inception_Multi_Modal_Final')

"""Model Testing on unseen test data"""

y_pred_test = loaded_model.predict([X_test_pic, X_test_news])

"""Observation of Predicted Results"""

y_pred_test

"""Use of Numpy functionality to convert float type metrics to 0's and 1's"""

import numpy as np

y_pred_test = np.where(y_pred_test > 0.5, 1, 0)

y_pred_test

"""Classification Report

"""

from sklearn import metrics
cm_RF=metrics.confusion_matrix(y_test,y_pred_test)
print(metrics.classification_report(y_test,y_pred_test))
print(cm_RF)

"""Graphical Representation of Confusion Matrix"""

import seaborn as sns
import matplotlib.pyplot as plt

#Plotting the confusion matrix

plt.figure(figsize=(7,7))
sns.heatmap(cm_RF, annot=True,cmap="Reds")
plt.title('Confusion Matrix')
plt.ylabel('Actual News')
plt.xlabel('Predicted News')

plt.show()