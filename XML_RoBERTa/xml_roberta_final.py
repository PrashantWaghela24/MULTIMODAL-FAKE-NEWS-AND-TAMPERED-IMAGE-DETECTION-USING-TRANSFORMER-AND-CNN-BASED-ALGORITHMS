# -*- coding: utf-8 -*-
"""XML_RoBERTa_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vPDFQeDqvQQlGWGW1EM8pojFdbDZ5ftk

Mounting Google Drive to access dataset and save created model
"""

from google.colab import drive
drive.mount('/content/drive')

"""Library Installation"""

!pip install tensorflow-text

"""Importing various necessary libraries"""

import tensorflow_text as text
import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
from sklearn import preprocessing
import keras
import tensorflow_hub as hub

"""Read the csv data file"""

# Read the tsv data file
df = pd.read_csv('/content/drive/MyDrive/FinalCleanData.csv')
#print(df)
df=pd.DataFrame(df)

"""Custom for loop to extract image path from Google Drive location"""

old_filelength=38520
train_split_val = 29699    #This value is 80% of entire dataset length 
FilePath_arr=[]
img_number_arr=[]
for i in range(0,len(df)):
    if((i <= train_split_val) & (df.iat[i,5] == 0)):
        FilePath_arr.append('/content/drive/MyDrive/images_resized/train_resized/fake_resized/image-' + str(df.iat[i,1]) + '.jpg')
        img_number_arr.append(df.iat[i,1])
    elif((i <= train_split_val) & (df.iat[i,5] == 1)):
        FilePath_arr.append('/content/drive/MyDrive/images_resized/train_resized/true_resized/image-' + str(df.iat[i,1]) + '.jpg')
        img_number_arr.append(df.iat[i,1])
    elif((i > train_split_val) & (df.iat[i,5] == 0)):
        FilePath_arr.append('/content/drive/MyDrive/images_resized/test_resized/fake_resized/image-' + str(df.iat[i,1]) + '.jpg')
        img_number_arr.append(df.iat[i,1])
    elif((i > train_split_val) & (df.iat[i,5] == 1)):
        FilePath_arr.append('/content/drive/MyDrive/images_resized/test_resized/true_resized/image-' + str(df.iat[i,1]) + '.jpg')
        img_number_arr.append(df.iat[i,1])

"""Creating new dataframe which includes the image path on google drive. This is created using the array generated in the above code."""

import pandas as pd
import numpy as np
image_number = np.array(img_number_arr)
Path = np.array(FilePath_arr)
new_df = pd.DataFrame({'image_number': image_number, 'Path': Path}, columns=['image_number', 'Path'])
print(new_df)

"""Inner joining the new_df with merged_df using the key 'image_number' to obtain a final dataframe."""

merged_df=pd.merge(left=new_df, right=df, on='image_number', how='inner')
print(merged_df)
#merged_df.to_csv('/content/drive/MyDrive/MergedData.csv')

"""Porter Stemming implementation to obtain clean pre-processed news text."""

#Importing necessary libraries

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras import layers

voc_size=10000

import nltk
import re
from nltk.corpus import stopwords
nltk.download('stopwords')


#News text is cleaned using regular expression which removes special characters from the text.
#Porter Stemming is used to remove stop words from the text to simplify it for the model.

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
corpus = []
for i in range(0, len(merged_df)):
    print(i)
    news = re.sub('[^a-zA-Z0-9]', ' ', merged_df.iat[i,5])
    news = news.lower()
    news = news.split()
    
    news = [ps.stem(word) for word in news if not word in stopwords.words('english')]
    news = ' '.join(news)
    corpus.append(news)

"""Saving Final Data Frame on Google Drive"""

merged_df['preprocessed_newsText']=pd.Series(corpus)
merged_df.head()
#merged_df.to_csv('/content/drive/MyDrive/FinalProcessed_MergedData.csv')

"""Reading Final Data Frame"""

import pandas as pd
import numpy as np

df_final = pd.read_csv('/content/drive/MyDrive/FinalProcessed_MergedData.csv')
print(df_final.head())

"""Dropping Unnecessary Columns"""

df_final.drop(['Unnamed: 0','index','domain','img_url','news_title'], inplace=True, axis=1)

df_final.head()

x=list(df_final['preprocessed_newsText'])

y=list(df_final['is_fake'])

df_final.head()

"""Importing necessary libraries"""

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot

le = preprocessing.LabelEncoder()
le.fit(y)

"""Data Splitting into train, validation, and test sets"""

x_enc = x
y_enc = y

x_train = np.asarray(x_enc[:28000])
y_train = np.asarray(y_enc[:28000])

x_val=np.asarray(x_enc[28000:34000])
y_val=np.asarray(y_enc[28000:34000])

x_test=np.asarray(x_enc[34000:37113])
y_test=np.asarray(y_enc[34000:37113])

"""Downloading the pretrained XML_RoBERTa preprocessor and encoder."""

#XML_RoBERTa
xml_RoBERTa_preprocess = hub.KerasLayer("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1")
xml_RoBERTa_encoder = hub.KerasLayer("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1")

"""Defining a BERT Model for Fake text classification"""

# Bert layers
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = xml_RoBERTa_preprocess(text_input)
outputs = xml_RoBERTa_encoder(preprocessed_text)

# Neural network layers
l = tf.keras.layers.Dropout(0.1, name="dropout")(outputs['pooled_output'])
l = tf.keras.layers.Dense(1, activation='sigmoid', name="output")(l)

# Use inputs and outputs to construct a final model
model = tf.keras.Model(inputs=[text_input], outputs = [l])

"""Checking Model Summary and various Parameters"""

model.summary()

"""Model Compilation"""

METRICS = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall')
]

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=METRICS)

"""Early Stopping and ModelCheckpoint Implementation to save the best model"""

# Create a model saving callback and train for 10 epochs

from keras.callbacks import ModelCheckpoint, EarlyStopping
import os
import scipy
import pandas as pd
import numpy as np

es = EarlyStopping(patience=3)
cp = ModelCheckpoint('/content/drive/MyDrive/model_XML_RoBERTa', save_best_only=True, save_weights_only=True)
cb=[es,cp]

"""Model Training Step"""

history=model.fit(x_train,y_train,epochs=10,batch_size=32, validation_data=(x_val, y_val), verbose=1, callbacks=[cb])

"""Loss and Accuracy Plots using MatPlotLib to observe graphical behaviour of loss and accuracy metrics"""

import matplotlib.pyplot as plt

# plot the loss
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.title("Training vs Validation Loss Plot")
plt.show()
plt.savefig('LossVal_loss')

# plot the accuracy
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend()
plt.title("Training vs Validation Accuracy Plot")
plt.show()
plt.savefig('AccVal_acc')

"""Model Saving on Google Drive"""

# save the model as a h5 file
from tensorflow.keras.models import load_model

model.save('/content/drive/MyDrive/XML_RoBERTa_Final')

"""Loading the Saved Model"""

from keras.models import load_model

loaded_model = load_model('/content/drive/MyDrive/XML_RoBERTa_Final')

"""Model Testing on unseen test data"""

y_pred_test = loaded_model.predict(x_test)

"""Observation of Predicted Results"""

y_pred_test

"""Use of Numpy functionality to convert float type metrics to 0's and 1's"""

import numpy as np

y_pred_test = np.where(y_pred_test > 0.5, 1, 0)

y_pred_test

"""Classification Report

"""

from sklearn import metrics
cm_RF=metrics.confusion_matrix(y_test,y_pred_test)
print(metrics.classification_report(y_test,y_pred_test))
print(cm_RF)

"""Graphical Representation of Confusion Matrix"""

import seaborn as sns
import matplotlib.pyplot as plt

#Plotting the confusion matrix

plt.figure(figsize=(7,7))
sns.heatmap(cm_RF, annot=True,cmap="Reds")
plt.title('Confusion Matrix')
plt.ylabel('Actual News')
plt.xlabel('Predicted News')

plt.show()